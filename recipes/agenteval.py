# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

import sys
import time

from typing import Union

import PIL

import torch

import pandas as pd
import json
from typing import Any, Dict, List
from collections import defaultdict
from lm_eval.evaluator_utils import (
    get_sample_size,
    get_task_list,
)

from lm_eval.evaluator import evaluate
from lm_eval.models.hf_vlms import HFMultimodalLM
from lm_eval.models.huggingface import HFLM
from lm_eval.tasks import get_task_dict, TaskManager
from lm_eval.utils import make_table
from omegaconf import DictConfig
from torchtune import config, training, utils
from torchtune.data import (
    format_content_with_images,
    left_pad_sequence,
    Message,
    padded_collate_tiled_images_and_mask,
)
from torchtune.generation import generate, sample
from torchtune.modules import TransformerDecoder
from torchtune.modules.common_utils import local_kv_cache
from torchtune.modules.model_fusion import DeepFusionModel
from torchtune.modules.transforms import Transform

from torchtune.modules.transforms.tokenizers import (
    HuggingFaceModelTokenizer,
    ModelTokenizer,
)
from torchtune.recipe_interfaces import EvalRecipeInterface
from torchtune.training import FullModelTorchTuneCheckpointer


class _LLMEvalWrapper(HFLM):
    """An EvalWrapper for EleutherAI's eval harness based on gpt-fast's
    EvalWrapper: https://github.com/pytorch-labs/gpt-fast/blob/main/eval.py.

    Note:
        This is for text-only decoder models.

    Args:
        model (TransformerDecoder): The model to evaluate.
        tokenizer (ModelTokenizer): Tokenizer associated with the model being evaluated.
            This should be the same tokenizer used when fine-tuning the model.
        device (torch.device): The device to use.
        max_seq_length (int): The maximum sequence length to use.
        batch_size (int): The batch size per GPU to use.
        dtype (torch.dtype): dtype for the model caches during generation.
        enable_kv_cache (bool): Whether to enable KV cache for generation.
    """

    def __init__(
        self,
        model: TransformerDecoder,
        tokenizer: ModelTokenizer,
        *,
        device: torch.device,
        max_seq_length: int = 4096,
        batch_size: int = 8,
        dtype: torch.dtype = torch.float32,
        enable_kv_cache: bool = True,
    ):
        # TODO (@joecummings): Remove this init function so we don't load in extraneous stuff
        super().__init__(pretrained="gpt2", device=str(device))
        self._model = model
        self._tokenizer = tokenizer
        self._max_seq_length = max_seq_length
        self._batch_size = batch_size
        self._dtype = dtype
        self._enable_kv_cache = enable_kv_cache

    @property
    def model(self):
        return self._model

    @property
    def eot_token_id(self):
        return self._tokenizer.eos_id

    @property
    def max_length(self):
        return self._max_seq_length

    @property
    def max_gen_toks(self):
        return 256

    @property
    def batch_size(self):
        return self._batch_size

    @property
    def device(self):
        return self._device

    @property
    def enable_kv_cache(self):
        return self._enable_kv_cache

    def tok_encode(self, text: str, **kwargs) -> list[int]:
        print("get in tok_encode") # Yes
        # Note on add_bos flag: setting to False as this gives better results, for example
        # +1% on truthfulqa_mc2 with a LoRA finetune. lit-gpt also sets this to False,
        # see https://github.com/Lightning-AI/lit-gpt/blob/main/eval/lm_eval_harness.py#L66,
        # though notably fast-gpt does the opposite
        # https://github.com/pytorch-labs/gpt-fast/blob/main/eval.py#L123.
        if isinstance(self._tokenizer, HuggingFaceModelTokenizer):
            return self._tokenizer.base_tokenizer.encode(
                text=text, add_bos=False, add_eos=False
            )
        return self._tokenizer.encode(text=text, add_bos=False, add_eos=False)

    def tok_batch_encode(
        self, text: list[str], left_truncate_len: int = None, **kwargs
    ) -> tuple[torch.Tensor, torch.Tensor]:
        print("get in tok_batch_encode") # No?
        tokenized_text = [self.tok_encode(x) for x in text]

        # pad left
        x = left_pad_sequence(
            [torch.tensor(x) for x in tokenized_text],
            batch_first=True,
            padding_value=self._tokenizer.pad_id,
        )

        # the harness will use left_truncate_len to indicate that the current batch
        # needs to be truncated to self.max_seq_len - self.max_gen_toks
        if left_truncate_len is not None:
            x = x[:, -left_truncate_len:]

        return x, torch.ones_like(x)  # return 'mask' b/c it's expected by the harness

    def tok_decode(self, tokens: Union[list[int], int], **kwargs) -> str:
        print("get in tok_decode") # No?
        if isinstance(tokens, int):
            tokens = [tokens]
        return self._tokenizer.decode(tokens)

    def _model_call(self, inps: torch.Tensor, **kwargs) -> torch.Tensor:
        print("get in _model_call") # Yes
        return self._model(inps)

    def apply_chat_template(
        self, chat_history: list[dict[str, str]], add_generation_prompt: bool = True
    ) -> str:
        if hasattr(self._tokenizer, "prompt_template"):
            return self._tokenizer.prompt_template(chat_history)
        if isinstance(self.tokenizer, HuggingFaceModelTokenizer):
            return self.tokenizer.render_template(chat_history)
        raise ValueError(
            "You can't use a tokenizer without a prompt template and apply_chat_template: True. "
            "Use HuggingFaceModelTokenizer if you do not require a custom one."
        )

    @torch.inference_mode()
    def _model_generate(
        self, context: torch.Tensor, **generation_kwargs
    ) -> torch.Tensor:
        print("get in _model_generate") # No? I guess its for gen job!
        bsz, seq_len = context.shape

        temperature = generation_kwargs.get("temperature", 0.0)
        do_sample = generation_kwargs.get("do_sample", False)
        if do_sample or temperature != 0.0:
            raise RuntimeError(
                "Any decoding strategy other than greedy is not supported."
            )

        # if we've recieved fewer than self._batch_size samples in the current
        # batch we need to pad the batch out. here we're padding the end of the
        # current batch to the correct length. this is because when we use static
        # KV-caches, the model will expect a fixed batch size for all samples.
        maybe_padded_context = torch.nn.functional.pad(
            context,
            (0, 0, 0, self._batch_size - bsz),
            value=self._tokenizer.eos_id,  # pad with one of the tokenizer's stop tokens so generation can stop early
        )
        with local_kv_cache(
            self.model,
            batch_size=self.batch_size,
            device=self.device,
            dtype=self._dtype,
            decoder_max_seq_len=self.max_length,
        ):
            toks, _ = generate(
                self.model,
                maybe_padded_context,
                max_generated_tokens=self.max_gen_toks,
                temperature=temperature,
                top_k=None,
                pad_id=self._tokenizer.pad_id,
                stop_tokens=self._tokenizer.stop_tokens,
            )
        return toks[:bsz]


class EleutherEvalRecipe(EvalRecipeInterface):
    """
    This recipe runs evaluation on a trained model using EleutherAI's eval harness.
    This assumes the user has the EleutherAI eval harness installed. See
    https://github.com/EleutherAI/lm-evaluation-harness for more details.

    Features:
        - Single GPU evaluation. Multi-GPU evaluation is currently not supported.
        - Quantization (for text-only models) is supported.
        - Any task from the EleutherAI eval harness

    We recommend launching evaluation using the tune CLI::

        tune run eleuther_eval --config eleuther_evaluation \
            tasks=["truthfulqa_mc2","hellaswag"] \
            limit=50 \
    """

    def __init__(self, cfg: DictConfig) -> None:
        # Double check we have the right Eval Harness version
        from importlib.metadata import version

        if version("lm-eval") < "0.4.5" or version("lm-eval") > "0.4.8":
            raise RuntimeError(
                "This recipe requires EleutherAI Eval Harness between v0.4.5 - 0.4.8."
                "Please install with `pip install lm-eval==0.4.8`"
            )

        # General variable initialization
        self.device = utils.get_device(device=cfg.device)
        self.dtype = training.get_dtype(dtype=cfg.dtype, device=self.device)
        self.logger = utils.get_logger(cfg.get("log_level", "info"))
        training.set_seed(
            seed=cfg.seed, debug_mode=cfg.get("cudnn_deterministic_mode", None)
        )

        # Eval specific variables
        self.limit = cfg.limit
        self.tasks = list(cfg.tasks)
        self.batch_size = cfg.batch_size
        self.enable_kv_cache = cfg.get("enable_kv_cache", True)
        self.include_path = cfg.get("include_path", None)
        self.apply_chat_template = cfg.get("chat_template", False)

        self.enable_multi_agent_eval = cfg.get("enable_multi_agent_eval", False)
        self.max_new_tokens = cfg.get("max_new_tokens", 512)
        self.temperature = cfg.get("temperature", 0.7)
        self.top_k = cfg.get("top_k", 50)

    def setup(self, cfg: DictConfig) -> None:
        # Initialize quantizer and quantization mode
        quantizer = config.instantiate(cfg.quantizer)
        quantization_mode = training.get_quantizer_mode(quantizer)

        # Load checkpoint
        checkpointer = config.instantiate(cfg.checkpointer)

        # Initialize model
        with training.set_default_dtype(self.dtype), self.device:
            model = config.instantiate(cfg.model)

        # Quantize model if requested
        if quantization_mode is not None:
            if not isinstance(checkpointer, FullModelTorchTuneCheckpointer):
                raise ValueError(
                    "Quantization is only supported for models quantized and saved with the "
                    "FullModelTorchTuneCheckpointer - please ensure you have quantized your "
                    "model and are using the quantized weights!"
                )
            if "qat" in quantization_mode:
                raise ValueError(
                    "You have specified a quantizer with 'QAT' - "
                    "QAT quantizers should only be used during quantization aware training "
                    "and when quantizing models. Please use the corresponding post-training "
                    "quantizer e.g. Int8DynActInt4WeightQuantizer for Int8DynActInt4WeightQATQuantizer."
                )
            model = quantizer.quantize(model)
            model = model.to(device=self.device, dtype=self.dtype)
            ckpt_dict = checkpointer.load_checkpoint(weights_only=False)[
                training.MODEL_KEY
            ]
            for k, v in ckpt_dict.items():
                ckpt_dict[k] = v.to(self.device)
            model.load_state_dict(ckpt_dict, assign=True)
        else:
            ckpt_dict = checkpointer.load_checkpoint()[training.MODEL_KEY]
            model.load_state_dict(ckpt_dict)

        # Load model weights into initialized model
        self.logger.info(f"Model is initialized with precision {self.dtype}.")

        # Put model in eval mode.
        # Note: This will not disable the dropout applied in SDPA,
        # see https://github.com/pytorch/pytorch/issues/124464
        model.eval()

        # Initialize tokenizer/transform
        model_transform = config.instantiate(cfg.tokenizer)

        # Finally, we setup the actual EvalWrapper class
        if isinstance(model, DeepFusionModel):
            pass # i dont care about vlm
            # eleuther_model_wrapper = _VLMEvalWrapper
            # if not self.enable_kv_cache:
            #     self.logger.debug(
            #         "Received enable_kv_cache=False, but KV cache is required for running "
            #         "multimodal generation in a timely manner. Setting enable_kv_cache=True."
            #     )
        elif isinstance(model, TransformerDecoder):
            eleuther_model_wrapper = _LLMEvalWrapper
        self.eleuther_model_wrapper = eleuther_model_wrapper(
            model,
            model_transform,
            device=self.device,
            max_seq_length=cfg.max_seq_length,
            batch_size=self.batch_size,
            dtype=self.dtype,
            enable_kv_cache=self.enable_kv_cache,
        )

    def _run_multi_agent_loglikelihood(self, reqs: List, instance_idx_to_task: Dict[int, Any]) -> List[torch.Tensor]:
        """
        å°ä¸€æ‰¹ loglikelihood è«‹æ±‚åŸ·è¡Œå¤šæ™ºèƒ½é«”å”ä½œè©•ä¼°ã€‚
        Args:
            reqs: List of Instance objects with request_type 'loglikelihood'.
            instance_idx_to_task: A dictionary mapping instance.idx to its parent Task.
        Returns:
            List of torch.Tensor, the log likelihoods for each request.
        """
        responses = []
        print(f"get in _run_multi_agent_loglikelihood, # of reqs: {len(reqs)}") # i guess 4
        for req in reqs:
            # ç²å–å¿…è¦çš„ä¿¡æ¯
            doc = req.doc
            # === é—œéµä¿®æ­£ï¼šä½¿ç”¨ req.idx ä½œç‚ºéµ ===
            task = instance_idx_to_task[req.idx] # é€™è£¡æ˜¯ idxï¼Œä¸æ˜¯ index
            choices = task.doc_to_choice(doc) # ç¾åœ¨ task æ˜¯æœ‰æ•ˆçš„

            # --- é—œéµä¿®æ”¹ï¼šç¢ºä¿ context_tensor æ˜¯ tensor ---
            # req.args[0] å¯èƒ½æ˜¯ str æˆ– torch.Tensor
            context_input = req.args[0]
            if isinstance(context_input, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç”¨ wrapper çš„ tok_encode å°‡å…¶è½‰æ›ç‚º tensor
                context_tensor = torch.tensor(
                    self.eleuther_model_wrapper.tok_encode(context_input),
                    device=self.device
                ).unsqueeze(0) # å¢åŠ  batch ç¶­åº¦
            elif isinstance(context_input, torch.Tensor):
                # å¦‚æœå·²ç¶“æ˜¯ tensorï¼Œç›´æ¥ä½¿ç”¨
                context_tensor = context_input.to(self.device)
            else:
                raise TypeError(f"Unexpected type for context: {type(context_input)}")

            # ç¾åœ¨ context_tensor è‚¯å®šæ˜¯ tensor äº†ï¼Œå¯ä»¥å®‰å…¨è§£ç¢¼
            context_str = self.eleuther_model_wrapper.tok_decode(context_tensor[0].tolist())

            # --- æ™ºèƒ½é«” 1: Thinker ---
            thinker_prompt = f"{context_str}\n\nPlease act as a thoughtful expert. Analyze the question and each option carefully. Explain your reasoning step by step for each option, and then give your best prediction (only the letter)."
            thinker_tensor = self.eleuther_model_wrapper.tok_encode(thinker_prompt)
            thinker_tensor = torch.tensor([thinker_tensor], device=self.device)
            with local_kv_cache(self.eleuther_model_wrapper.model, batch_size=1, device=self.device, dtype=self.dtype, decoder_max_seq_len=self.eleuther_model_wrapper.max_length):
                thinker_output, _ = generate(
                    self.eleuther_model_wrapper.model,
                    thinker_tensor,
                    max_generated_tokens=self.max_new_tokens,
                    temperature=self.temperature,
                    top_k=self.top_k,
                    pad_id=self.eleuther_model_wrapper._tokenizer.pad_id,
                    stop_tokens=self.eleuther_model_wrapper._tokenizer.stop_tokens,
                )
            thinker_response = self.eleuther_model_wrapper.tok_decode(thinker_output[0].tolist())
            print("finish agent 1")
            # --- æ™ºèƒ½é«” 2: Critic ---
            critic_prompt = f"{context_str}\n\nHere is an analysis from a fellow expert:\n{thinker_response}\n\nPlease act as a critical reviewer. Identify any flaws, biases, or errors in the above analysis. Do you agree with the final prediction? If not, explain why and provide your own reasoning. Then give your own prediction (only the letter)."
            critic_tensor = self.eleuther_model_wrapper.tok_encode(critic_prompt)
            critic_tensor = torch.tensor([critic_tensor], device=self.device)
            with local_kv_cache(self.eleuther_model_wrapper.model, batch_size=1, device=self.device, dtype=self.dtype, decoder_max_seq_len=self.eleuther_model_wrapper.max_length):
                critic_output, _ = generate(
                    self.eleuther_model_wrapper.model,
                    critic_tensor,
                    max_generated_tokens=self.max_new_tokens,
                    temperature=self.temperature,
                    top_k=self.top_k,
                    pad_id=self.eleuther_model_wrapper._tokenizer.pad_id,
                    stop_tokens=self.eleuther_model_wrapper._tokenizer.stop_tokens,
                )
            critic_response = self.eleuther_model_wrapper.tok_decode(critic_output[0].tolist())
            print("finish agent 2")
            # --- æ™ºèƒ½é«” 3: Final Judge ---
            judge_prompt = f"{context_str}\n\nHere are two expert analyses:\n1. Thinker: {thinker_response}\n2. Critic: {critic_response}\n\nPlease act as the final judge. Synthesize both viewpoints. Which prediction do you find more convincing? Give a final, definitive prediction (only the letter)."
            judge_tensor = self.eleuther_model_wrapper.tok_encode(judge_prompt)
            judge_tensor = torch.tensor([judge_tensor], device=self.device)
            with local_kv_cache(self.eleuther_model_wrapper.model, batch_size=1, device=self.device, dtype=self.dtype, decoder_max_seq_len=self.eleuther_model_wrapper.max_length):
                judge_output, _ = generate(
                    self.eleuther_model_wrapper.model,
                    judge_tensor,
                    max_generated_tokens=64,
                    temperature=0.0, # Greedy
                    pad_id=self.eleuther_model_wrapper._tokenizer.pad_id,
                    stop_tokens=self.eleuther_model_wrapper._tokenizer.stop_tokens,
                )
            judge_prediction = self.eleuther_model_wrapper.tok_decode(judge_output[0].tolist()).strip()
            print("finish agent 3")
            # --- è§£ææœ€çµ‚é æ¸¬ä¸¦ç”Ÿæˆ log likelihoods ---
            num_choices = len(choices)
            fake_log_likelihoods = torch.full((1, num_choices), -1000.0, device=self.device)
            import re
            match = re.search(r'\b([A-D])\b', judge_prediction)
            if match:
                chosen_idx = ord(match.group(1)) - ord('A')
                if 0 <= chosen_idx < num_choices:
                    fake_log_likelihoods[0, chosen_idx] = 0.0
            else:
                fake_log_likelihoods[0, 0] = 0.0 # é»˜èª

            # é‡è¤‡ä»¥åŒ¹é… req.repeats
            for _ in range(req.repeats):
                responses.append(fake_log_likelihoods)

        return responses

    def agenteval(self) -> None:
        # === æ–°çš„å¤šæ™ºèƒ½é«”è©•ä¼°æµç¨‹ ===
        task_manager = TaskManager(include_path=self.include_path)
        task_dict = get_task_dict(self.tasks, task_manager)
        eval_tasks = get_task_list(task_dict) # å¾ evaluator.py ä¾†çš„

        # 2. Build All Requests (é—œéµæ­¥é©Ÿ)
        requests = defaultdict(list)
        # === ä¿®æ­£ï¼šä½¿ç”¨ instance.idx ä½œç‚ºéµ ===
        instance_idx_to_task = {}
        
        for task_output in eval_tasks:
            task = task_output.task
            limit = get_sample_size(task, self.limit)
            # é€™æœƒå¡«å…… task.instances
            task.build_all_requests(
                limit=limit,
                rank=0, # å‡è¨­å–®GPU
                world_size=1,
                cache_requests=False,
                rewrite_requests_cache=False,
                system_instruction=None,
                apply_chat_template=self.apply_chat_template,
                fewshot_as_multiturn=False,
                chat_template=self.eleuther_model_wrapper.apply_chat_template if self.apply_chat_template else None,
                tokenizer_name="",
            )
            # æŒ‰ reqtype åˆ†é¡
            for instance in task.instances:
                requests[instance.request_type].append(instance)
                # === é—œéµä¿®æ­£ï¼šä½¿ç”¨ instance.idx ä½œç‚ºéµ ===
                instance_idx_to_task[instance.idx] = task # é€™è£¡æ˜¯ idxï¼Œä¸æ˜¯ index

        # 3. åŸ·è¡Œå¤šæ™ºèƒ½é«”è©•ä¼° (æ¨¡æ“¬ getattr(lm, reqtype)(cloned_reqs))
        all_responses = {}
        for reqtype, reqs in requests.items():
            self.logger.info(f"Running multi-agent {reqtype} evaluation")
            if reqtype == "loglikelihood":
                responses = self._run_multi_agent_loglikelihood(reqs, instance_idx_to_task)
            else:
                # å°æ–¼å…¶ä»–é¡å‹ï¼ˆå¦‚ generate_untilï¼‰ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹
                cloned_reqs = [req for req in reqs for _ in range(req.repeats)]
                inps = torch.stack([req.args[0] for req in cloned_reqs]) # å‡è¨­ args[0] æ˜¯ context_tensor
                outs = self.eleuther_model_wrapper._model_call(inps)
                responses = [outs[i] for i in range(len(cloned_reqs))]
            all_responses[reqtype] = responses

        # 4. å°‡éŸ¿æ‡‰å¡«å› instances
        response_idx = 0
        for reqtype, reqs in requests.items():
            for req in reqs:
                for _ in range(req.repeats):
                    req.resps.append(all_responses[reqtype][response_idx])
                    response_idx += 1

        # 5. å¾Œè™•ç†å’Œçµæœèšåˆ (ç›´æ¥è¤‡ç”¨ evaluator.py çš„ä»£ç¢¼)
        # é€™éƒ¨åˆ†å¯ä»¥ç›´æ¥èª¿ç”¨ evaluator.py çš„ consolidate_results ç­‰å‡½æ•¸
        for task_output in eval_tasks:
            task = task_output.task
            task.apply_filters()
            # ... (process_results, calculate_aggregate_metric ç­‰)
            # ç”±æ–¼ä»£ç¢¼è¤‡é›œï¼Œæˆ‘å€‘å¯ä»¥å…ˆåªå¯¦ç¾ loglikelihood çš„å¤šæ™ºèƒ½é«”æµç¨‹ï¼Œä¸¦æ‰‹å‹•æ§‹å»ºè¼¸å‡ºã€‚

        # 6. æ§‹å»ºæœ€çµ‚è¼¸å‡ºä¸¦èª¿ç”¨ log_detailed_results
        # ç”±æ–¼å®Œå…¨è¤‡è£½å¾Œè™•ç†å¾ˆè¤‡é›œï¼Œæˆ‘å€‘å¯ä»¥å…ˆæ‰‹å‹•æ§‹å»ºä¸€å€‹ç°¡åŒ–çš„ output å­—å…¸
        output = {
            "results": {},
            "configs": {task_output.task_name: task_output.task.config for task_output in eval_tasks},
            "versions": {task_output.task_name: getattr(task_output.task, "VERSION", "N/A") for task_output in eval_tasks},
            "samples": {}
        }
        # å¡«å…… samples
        for task_output in eval_tasks:
            task_name = task_output.task_name
            output["samples"][task_name] = []
            instances_by_doc_id = defaultdict(list)
            for instance in task_output.task.instances:
                instances_by_doc_id[instance.doc_id].append(instance)
            for doc_id, instances in instances_by_doc_id.items():
                instances.sort(key=lambda x: x.idx)
                sample = {
                    "doc": instances[0].doc,
                    "arguments": [req.args for req in instances],
                    "resps": [req.resps for req in instances],
                    "filtered_resps": [req.filtered_resps[list(req.filtered_resps.keys())[0]] for req in instances] if instances[0].filtered_resps else [req.resps for req in instances],
                }
                output["samples"][task_name].append(sample)
        print(output)
        # èª¿ç”¨ä½ å·²æœ‰çš„æ—¥èªŒå‡½æ•¸
        df = log_detailed_results(
            output,
            output_file="agenteval_detailed.json",
            md_output_file="agenteval_report.md",
            mode="loglikelihood"
        )

        # é¡¯ç¤ºå‰ 5 ç­†
        self.logger.info("\n\nğŸ“Œ First 5 detailed results:")
        self.logger.info("\n" + df[["question", "is_correct"]].head(5).to_string(index=False))

        formatted_output = make_table(output)
        self.logger.info(f"\n\n{formatted_output}\n")


    def evaluate(self) -> None:
        # Initialize tasks for the harness
        task_manager = TaskManager(include_path=self.include_path)
        task_dict = get_task_dict(self.tasks, task_manager)

        # Run evaluation
        t0 = time.time()
        self.logger.info(f"Running evaluation on the following tasks: {self.tasks}")
        self.logger.info(task_dict)
        self.logger.info(self.apply_chat_template)
        output = evaluate(
            self.eleuther_model_wrapper,
            task_dict,
            apply_chat_template=self.apply_chat_template,
            limit=self.limit,
            write_out=True,

        )
        t1 = time.time() - t0

        # Log metrics
        self.logger.info(f"Eval completed in {t1:.02f} seconds.")
        if self.device.type != "cpu" and self.device.type != "mps":
            torch_device = utils.get_torch_device_namespace()
            self.logger.info(
                f"Max memory allocated: {torch_device.max_memory_allocated() / 1e9:.02f} GB"
            )

        # print(output)
        df = log_detailed_results(
            output,
            output_file="eval_detailed.json",
            md_output_file="eval_report.md",
            mode="loglikelihood"
        )

        # é¡¯ç¤ºå‰ 5 ç­†
        self.logger.info("\n\nğŸ“Œ First 5 detailed results:")
        self.logger.info("\n" + df[["question", "is_correct"]].head(5).to_string(index=False))

        formatted_output = make_table(output)
        self.logger.info(f"\n\n{formatted_output}\n")


def log_detailed_results(
    output: Dict[str, Any],
    output_file: str = "detailed_results.json",
    md_output_file: str = "detailed_report.md",
    mode: str = "loglikelihood",  # æˆ– "generation"
) -> pd.DataFrame:
    """
    å¾ EleutherAI eval harness çš„ output ä¸­æå–æ¯ä¸€é¡Œçš„è©³ç´°çµæœï¼Œ
    åŒ…æ‹¬å•é¡Œã€é¸é …ã€log likelihoodã€æ˜¯å¦æ­£ç¢ºï¼Œä¸¦è¼¸å‡ºç‚º JSON å’Œç¾è§€ Markdownã€‚
    æ”¯æ´ loglikelihood å’Œ future generation modeã€‚
    """
    records = []
    prompts_and_responses = []  # ç”¨æ–¼ Markdown çš„ rich å±•ç¤º

    for task_name, task_data in output["samples"].items():
        config = output["configs"][task_name]
        version = output["versions"].get(task_name, "N/A")

        for idx, sample in enumerate(task_data):
            doc = sample["doc"]
            question = doc["question"]
            choices = doc.get("mc1_targets", {}).get("choices", doc.get("mc2_targets", {}).get("choices", []))
            labels = doc.get("mc1_targets", {}).get("labels", doc.get("mc2_targets", {}).get("labels", []))
            true_indices = [i for i, lbl in enumerate(labels) if lbl == 1]

            # === æ ¹æ“š mode åˆ†æ”¯è™•ç† ===
            if mode == "loglikelihood":
                try:
                    pred_log_likelihoods = [resp[0][0] for resp in sample["resps"]]
                except ValueError: # with my agenteval... only one element tensors can be converted to Python scalars
                    first_resp = sample["resps"][0]  # å‡è¨­æ‰€æœ‰éŸ¿æ‡‰ç›¸åŒ
                    # first_resp æ‡‰è©²æ˜¯åƒ [tensor([[ll_A, ll_B, ...]])] é€™æ¨£çš„åˆ—è¡¨
                    # æ‰€ä»¥ first_resp[0] æ˜¯ tensor([[ll_A, ll_B, ...]])
                    # æˆ‘å€‘éœ€è¦ squeeze æˆä¸€ç¶­ä¸¦è½‰æ›ç‚º Python list
                    pred_log_likelihoods_tensor = first_resp[0].squeeze(0)  # å½¢ç‹€: (num_choices,)
                    pred_log_likelihoods = pred_log_likelihoods_tensor.tolist() # è½‰æ›ç‚º Python list
                pred_idx = int(torch.argmax(torch.tensor(pred_log_likelihoods)).item())
                is_correct = pred_idx in true_indices
                probs = torch.softmax(torch.tensor(pred_log_likelihoods), dim=0)
                max_prob = probs[pred_idx].item()

                # æå– promptï¼šä½¿ç”¨ arguments[0][0]ï¼ˆæ‰€æœ‰é¸é …å…±äº«åŒä¸€å€‹ promptï¼‰
                prompt = sample["arguments"][0][0].strip()
                generated_text = choices[pred_idx]
                log_likelihoods = {f"choice_{i}": float(ll) for i, ll in enumerate(pred_log_likelihoods)}

            elif mode == "generation":
                # TODO: æœªä¾†å¯¦ç¾è‡ªç”±ç”Ÿæˆçš„åˆ†æ
                # å‡è¨­ output æ ¼å¼æœƒæœ‰: sample["generated_text"]
                raise NotImplementedError("Generation mode not implemented yet. Use mode='loglikelihood'.")

            else:
                raise ValueError(f"Unsupported mode: {mode}")

            # === æº–å‚™ JSON è¨˜éŒ„ï¼ˆå®Œæ•´è³‡æ–™ï¼‰===
            records.append({
                "task": task_name,
                "version": version,
                "question": question,
                "choices": choices,
                "correct_indices": true_indices,
                "model_predicted_index": pred_idx,
                "model_predicted_text": choices[pred_idx],
                "is_correct": is_correct,
                "max_prob": round(max_prob, 4) if mode == "loglikelihood" else None,
                "log_likelihoods": log_likelihoods if mode == "loglikelihood" else None,
                "prompt": prompt if mode == "loglikelihood" else None,
                "generated_text": generated_text if mode == "loglikelihood" else None,
            })

            # === æº–å‚™ Markdown ç”¨çš„ rich å±•ç¤º ===
            result = "âœ… Correct" if is_correct else "âŒ Incorrect"
            short_question = question[:50] + "..." if len(question) > 50 else question

            prompts_and_responses.append({
                "index": idx,
                "task": task_name,
                "short_question": short_question,
                "result": result,
                "prompt": prompt,
                "predicted_text": choices[pred_idx],
                "correct_indices": true_indices,
                "choices": choices,
            })

    # === ä¿å­˜ JSON ===
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(records, f, indent=2, ensure_ascii=False)

    # === ç”Ÿæˆ Markdown å ±å‘Š ===
    _generate_detailed_markdown_report(prompts_and_responses, md_output_file, records)

    # === è¿”å› DataFrame ===
    df = pd.DataFrame(records)
    print(f"\nâœ… Detailed results saved to:")
    print(f"   - {output_file}")
    print(f"   - {md_output_file}")
    print(f"   Total samples: {len(df)}")

    return df

def _generate_detailed_markdown_report(data: List[Dict], md_file: str, full_records: List[Dict]):
    """ç”Ÿæˆç¾è§€çš„ Markdown å ±å‘Šï¼ŒåŒ…å«æ‘˜è¦è¡¨æ ¼å’Œæ¯é¡Œè©³ç´° prompt åˆ†æã€‚"""
    with open(md_file, "w", encoding="utf-8") as f:
        f.write("# ğŸ“Š Evaluation Detailed Report\n\n")

        # === æ‘˜è¦çµ±è¨ˆ ===
        total = len(data)
        correct = sum(1 for d in data if "âœ…" in d["result"])
        accuracy = correct / total if total > 0 else 0
        f.write(f"**Summary**: {correct}/{total} correct ({accuracy:.1%})\n\n")

        # === æ‘˜è¦è¡¨æ ¼ï¼ˆç°¡æ½”ï¼‰===
        f.write("## ğŸ“ˆ Summary Table\n\n")
        f.write("| # | Task | Question | Result |\n")
        f.write("|---|------|----------|--------|\n")
        for d in data:
            f.write(f"| {d['index']} | `{d['task']}` | {d['short_question']} | {d['result']} |\n")
        f.write("\n")

        # === æ¯é¡Œè©³ç´°åˆ†æ ===
        f.write("## ğŸ§© Detailed Analysis\n\n")
        for d in data:
            f.write(f"### Question {d['index']} ({d['task']})\n")
            f.write(f"**Result**: {d['result']}\n\n")
            f.write("**Question**: " + d["short_question"] + "\n\n")

            # Correct choices
            correct_choices = [d["choices"][i] for i in d["correct_indices"]]
            f.write("**Correct Answer(s)**:\n")
            for c in correct_choices:
                f.write(f"- âœ… `{c}`\n")
            f.write("\n")

            # Model prediction
            f.write(f"**Model Prediction**: `{d['predicted_text']}`\n\n")

            # Prompt (ç”¨ collapsible block æ”¶èµ·ä¾†)
            f.write("<details>\n")
            f.write("<summary>ğŸ” Show Prompt</summary>\n\n")
            f.write("```\n")
            f.write(d["prompt"].replace("`", "\\`") + "\n")
            f.write("```\n")
            f.write("</details>\n\n")

            f.write("---\n\n")

    print(f"âœ… Markdown report generated at {md_file}")


@config.parse
def recipe_main(cfg: DictConfig) -> None:
    """Entry point for the recipe."""
    config.log_config(recipe_name="EleutherEvalRecipe", cfg=cfg)
    recipe = EleutherEvalRecipe(cfg=cfg)
    recipe.setup(cfg=cfg)
    recipe.evaluate()
    recipe.agenteval()


if __name__ == "__main__":
    sys.exit(recipe_main())
